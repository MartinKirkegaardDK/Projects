{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data used for this project is a fake news dataset which can be found on huggingface under that path: GonzaloA/fake_news. The data is described as a \"mix of other datasets which are the same scope, the Fake News\". Unfullfilled with this rather vague description, we sought to find additional information regarding the data and found this kaggle dataset: \n",
    "\n",
    "https://www.kaggle.com/datasets/emineyetm/fake-news-detection-datasets?fbclid=IwZXh0bgNhZW0CMTAAAR3jgQPMw-W_8Vbi2k7XOmm7Dt50Tr45yiFsk5GH_tTsbY3JpJlMJAvDHuc_aem_Af7stCPp4fcGMauSxhPBOO7Tc5g8CxOy4vZUSlkRxxlb6zeVxi-KCFfi8TAfe9i2wwdsLD-cci7LhoeMokOvypUy\n",
    "\n",
    "We wanted to test for simmilarity and found that 97.42% of our training data titles are identical to data found in the kaggle dataset (see code below). An interesting outcome of the kaggle data is that all fake news articles comes from **websites** flagged by Politifact and not individual articles, and the true articles comes from Reuters. This has the effect that the latent space for our model is likely whether or not an article is published by Reuters and not the intented fake news detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Fake.csv\")\n",
    "true_data = pd.read_csv(\"True.csv\")\n",
    "#The Gonzaloa is the dataset we have used for our analysis\n",
    "our_data = load_dataset('GonzaloA/fake_news')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled = data.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "d = defaultdict(dict)\n",
    "c = 0\n",
    "#A simple nested loop that loops through all the titles in both datasets.\n",
    "for counter, text in enumerate(our_data[\"train\"][\"title\"]):\n",
    "    found = False\n",
    "    for text_2 in sampled:\n",
    "        if text in text_2:\n",
    "            d[counter][\"our_data\"] = text\n",
    "            d[counter][\"sampled\"] = text_2\n",
    "            d[counter][\"missing\"] = False\n",
    "            found = True\n",
    "            #When we encounter a match, we simply break the loop for efficiency \n",
    "            break\n",
    "    if not found:\n",
    "        d[counter][\"our_data\"] = None\n",
    "        d[counter][\"sampled\"] = text\n",
    "        d[counter][\"missing\"] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(d).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It takes some time to run, thus we have split the processing up into two parts, true and false.\n",
    "#We have already found all the indexes for the label true\n",
    "with open(\"true.txt\",\"r\") as f:\n",
    "    x = f.read()\n",
    "    true = set([int(x) for x in x.split(\"\\n\")])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We merge the data into one set for quick lookup\n",
    "merged = true.union(set(df[df[\"missing\"] == False].index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These are the indexes in our data which is NOT present in the kaggle dataset\n",
    "li = []\n",
    "for i in range(max(merged)):\n",
    "    if i not in merged:\n",
    "        li.append(i)\n",
    "print(100-len(li)/(true_data.shape[0] + data.shape[0])*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection on fake news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fake news has many definitions, ranging from being factually incorrect to misleading, which makes it hard when quantifying results and cross examining results between different studies, since the latent space can be different dependent on the definition of the task. The problem with using objective truth as the definition, that truth can vary depending on culture and context. An actor in a conflict can be seen as the good freedome fighter towards an opressive regime by one side, and as a terrorist on the other side. Lastly, the truth can also change over time, which means that models have to be retrained with up to date information constantly to be able to combat the fake news within the catagory factual correct. Using the definition of misleading would be an inherently easier task since the model would not have to have a world view in order to classify. It should be solveable by using information only present in the text and comparing that to the title. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Related work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Recent studies have shown that fake news propagates through social media at unprecedented speeds. This was observed to happen during the emergence of COVID-19, thus the need to quickly detect and mitigate the spreading of fake news is more important than ever s[1]. \n",
    "\n",
    "Many definitions are presented, ranging from being factually incorrect to misleading, and unfortunately, our data source has not specified which definition they use. This makes it harder to interpret why a model predicted as it did, since we do not know if the data contains mostly stories conflicting with reality, or simply written by an overselling journalist. -->\n",
    "\n",
    "Related work\n",
    "This paper, written by Shaina Raza & Chen Ding, uses META's BART language model trained on two data sets: NELA-GT-19, which are news articles sourced from multiple sites, and Fakeddit, which is a multimodal dataset from Reddit, consisting of both images and text. The datasets used had more than a binary score, it included labels such as mixed, which is when there is a disagrement whether something is true or false, and categories such as satire into a single category Fake. They discuss their approach of continuously updating the model's training data to retrain the model and stay on top of relevant news. They also assert that freezing a model's weights can quickly make the model outdated since they don't generalize well to future events. Finally, they report an accuracy of 74.89%.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "s[1] = https://link.springer.com/article/10.1007/s41060-021-00302-z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "s[2] = https://arxiv.org/pdf/2101.00180.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ne_chunk, pos_tag, word_tokenize\n",
    "import nltk\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "from datasets import load_dataset\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(plt.style.available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('dark_background')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset('GonzaloA/fake_news')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(100)\n",
    "\n",
    "print(true[np.random.randint(1, len(true))]['text'])\n",
    "print()\n",
    "print(true[np.random.randint(1, len(true))]['text'])\n",
    "print()\n",
    "print(true[np.random.randint(1, len(true))]['text'])\n",
    "print()\n",
    "print(true[np.random.randint(1, len(true))]['text'])\n",
    "print()\n",
    "print(true[np.random.randint(1, len(true))]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(100)\n",
    "\n",
    "print(fake[np.random.randint(1, len(fake))]['text'])\n",
    "print()\n",
    "print(fake[np.random.randint(1, len(fake))]['text'])\n",
    "print()\n",
    "print(fake[np.random.randint(1, len(fake))]['text'])\n",
    "print()\n",
    "print(fake[np.random.randint(1, len(fake))]['text'])\n",
    "print()\n",
    "print(fake[np.random.randint(1, len(fake))]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ratios(first: list, other: list) -> dict:\n",
    "    '''\n",
    "    returns the frequency for each element in first divided by its frequency in other\n",
    "        - meant to show which elements are overrepresented in first compared to other\n",
    "    '''\n",
    "    first_counts = Counter(first)\n",
    "    other_counts = Counter(other)\n",
    "\n",
    "    ratios = {}\n",
    "\n",
    "    for item in first_counts:\n",
    "        if item in other_counts:\n",
    "            ratios[item] = first_counts[item] / other_counts[item]\n",
    "        else:\n",
    "            # to avoid division by zero, pretends items absent in other occur once instead\n",
    "            ratios[item] = first_counts[item] \n",
    "\n",
    "    return ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splits up data by label\n",
    "fake = []\n",
    "true = []\n",
    "for article in data['train']:\n",
    "    if article['label'] == 1: # label == true\n",
    "        true.append(article)\n",
    "    else:# label == fake\n",
    "        fake.append(article)\n",
    "\n",
    "\n",
    "# merges and tokenizes the text of the fake/non-fake news\n",
    "true_combined = ' '.join([article['text'] for article in true])\n",
    "fake_combined = ' '.join([article['text'] for article in fake])\n",
    "true_tokens = word_tokenize(true_combined)\n",
    "fake_tokens = word_tokenize(fake_combined)\n",
    "\n",
    "# computes token frequency in fake news / frequency in non-fake news (and vice versa) and sorts by ratio\n",
    "true_ratios = compute_ratios(true_tokens, fake_tokens)\n",
    "fake_ratios = compute_ratios(fake_tokens, true_tokens)\n",
    "true_ratios_sorted = sorted(list(true_ratios.items()), key=lambda x: x[1], reverse=True)\n",
    "fake_ratios_sorted = sorted(list(fake_ratios.items()), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepares token ratios for plotting\n",
    "\n",
    "# only plots top 10 highest ratios\n",
    "n_tokens = 10\n",
    "\n",
    "x_true = [token for token, _  in true_ratios_sorted[:n_tokens]]\n",
    "x_fake = [token for token, _  in fake_ratios_sorted[:n_tokens]]\n",
    "x_true.reverse()\n",
    "x_fake.reverse()\n",
    "\n",
    "y_true = [ratio for _, ratio in true_ratios_sorted[:n_tokens]]\n",
    "y_fake = [ratio for _, ratio in fake_ratios_sorted[:n_tokens]]\n",
    "y_true.reverse()\n",
    "y_fake.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples article to save compute time on the following\n",
    "size = 400\n",
    "np.random.seed(100)\n",
    "fake_sample = np.random.choice(fake, size=size)\n",
    "true_sample = np.random.choice(true, size=size)\n",
    "\n",
    "# does NER on non-fake news, storing all named entities in list\n",
    "true_NE = []\n",
    "true_combined = ' '.join([article['text'] for article in true_sample])\n",
    "tags = pos_tag(word_tokenize(true_combined))\n",
    "chunks = ne_chunk(tags)\n",
    "n_chunks = len(chunks)\n",
    "for i, chunk in enumerate(chunks):\n",
    "    if hasattr(chunk, 'label'):\n",
    "        true_NE.append(' '.join([word[0] for word in chunk]))\n",
    "\n",
    "# does NER on fake news, storing all named entities in list\n",
    "fake_NE = []\n",
    "fake_combined = ' '.join([article['text'] for article in fake_sample])\n",
    "tags = pos_tag(word_tokenize(fake_combined))\n",
    "chunks = ne_chunk(tags)\n",
    "n_chunks = len(chunks)\n",
    "for i, chunk in enumerate(chunks):\n",
    "    if hasattr(chunk, 'label'):\n",
    "        fake_NE.append(' '.join([word[0] for word in chunk]))\n",
    "\n",
    "\n",
    "# computes named entity frequency in fake news / frequency in non-fake news (and vice versa) and sorts by ratio\n",
    "true_ratios_NE = compute_ratios(true_NE, fake_NE)\n",
    "fake_ratios_NE = compute_ratios(fake_NE, true_NE)\n",
    "true_ratios_sorted_NE = sorted(list(true_ratios_NE.items()), key=lambda x: x[1], reverse=True)\n",
    "fake_ratios_sorted_NE = sorted(list(fake_ratios_NE.items()), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepares named entity ratios for plotting\n",
    "\n",
    "# only plots top 10 highest ratios\n",
    "n_entities = 10\n",
    "\n",
    "x_true_NE = [entities for entities, _  in true_ratios_sorted_NE[:n_entities]]\n",
    "x_fake_NE = [entities for entities, _  in fake_ratios_sorted_NE[:n_entities]]\n",
    "x_true_NE.reverse()\n",
    "x_fake_NE.reverse()\n",
    "\n",
    "y_true_NE = [ratio for _, ratio in true_ratios_sorted_NE[:n_entities]]\n",
    "y_fake_NE = [ratio for _, ratio in fake_ratios_sorted_NE[:n_entities]]\n",
    "y_true_NE.reverse()\n",
    "y_fake_NE.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots all the ratios\n",
    "fig, axs = plt.subplots(figsize=(12,4), nrows=2, ncols=2)\n",
    "\n",
    "axs = axs.flatten()\n",
    "\n",
    "axs[0].barh(x_true, y_true)\n",
    "axs[0].set_title('Tokens overrepresented in non-fake news')\n",
    "axs[0].tick_params('y', labelsize=9)\n",
    "\n",
    "axs[1].barh(x_fake, y_fake)\n",
    "axs[1].set_title('Tokens overrepresented in fake news')\n",
    "axs[1].tick_params('y', labelsize=9)\n",
    "\n",
    "axs[2].barh(x_true_NE, y_true_NE)\n",
    "axs[2].set_title('Named Entities overrepresented in non-fake news')\n",
    "axs[2].tick_params('y', labelsize=9)\n",
    "\n",
    "axs[3].barh(x_fake_NE, y_fake_NE)\n",
    "axs[3].set_title('Named Entities overrepresented in fake news')\n",
    "axs[3].tick_params('y', labelsize=9)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig('figs/overrepresented.png', dpi=300)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('fake_tokens.txt', 'a') as f:\n",
    "#    for token in sorted_:\n",
    "#        f.write(f'{token[0]}, {token[1][0]}, {token[1][1][:10]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------- splits up data by label and stores index --------\n",
    "fake = []\n",
    "true = []\n",
    "\n",
    "for i, article in enumerate(data['train']):\n",
    "    \n",
    "    # saving index for further analysis\n",
    "    info = {'text': article['text'], 'index': i}\n",
    "\n",
    "    if article['label'] == 0: # label == fake\n",
    "        fake.append(info)\n",
    "\n",
    "    else: # label == true\n",
    "        true.append(info)\n",
    "\n",
    "\n",
    "# --------- tokenizes the text of the fake news and counts occurrences ----------- \n",
    "fake_counts = defaultdict(lambda: {'count': 0, 'articles': []})\n",
    "\n",
    "for info in fake:\n",
    "    text = info['text']\n",
    "    index = info['index'] # stores document index for future analysis\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    for token in tokens:\n",
    "        fake_counts[token]['count'] += 1\n",
    "        fake_counts[token]['articles'].append(index)\n",
    "\n",
    "\n",
    "# -------- tokenizes the text of non-fake news and counts occurrences -----------\n",
    "true_combined = ' '.join([article['text'] for article in true])\n",
    "true_tokens = word_tokenize(true_combined)\n",
    "true_amounts = Counter(true_tokens)\n",
    "\n",
    "\n",
    "# --------- computes token frequency in fake news / frequency in non-fake news --------\n",
    "fake_tf = {}\n",
    "\n",
    "for token in fake_counts:\n",
    "    count = fake_counts[token]['count']\n",
    "    articles = fake_counts[token]['articles']\n",
    "    if token in true_amounts:\n",
    "        fake_tf[token] = {'freq': count / true_amounts[token], \n",
    "                          'articles': articles}\n",
    "    else:\n",
    "        # pretends tokens not appearing in true appear once (to avoid dividing by zero)\n",
    "        fake_tf[token] = {'freq': count, \n",
    "                          'articles': articles}\n",
    "\n",
    "\n",
    "# -------- sorts and prints tokens with highest value = most particular to fake news --------\n",
    "sorted_ = sorted(list(fake_tf.items()), key=lambda x: x[1]['freq'], reverse=True)\n",
    "for token, info in sorted_:\n",
    "    print(token, info['freq'], info['articles'][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tokens = 10\n",
    "\n",
    "x = [token for token, _  in sorted_[:20]]\n",
    "y = [articles['freq'] for _, articles in sorted_[:20]]\n",
    "\n",
    "x.reverse()\n",
    "y.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,12))\n",
    "\n",
    "ax.barh(x, y)\n",
    "ax.set_xticklabels(y, rotation=50, rotation_mode='anchor', ha='right')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 400\n",
    "fake_sample = np.random.choice(fake, size=size)\n",
    "true_sample = np.random.choice(true, size=size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_NE = []\n",
    "fake_combined = ' '.join([article['text'] for article in fake_sample])\n",
    "tags = pos_tag(word_tokenize(fake_combined))\n",
    "chunks = ne_chunk(tags)\n",
    "n_chunks = len(chunks)\n",
    "for i, chunk in enumerate(chunks):\n",
    "    if hasattr(chunk, 'label'):\n",
    "        fake_NE.append(' '.join([word[0] for word in chunk]))\n",
    "\n",
    "\n",
    "true_NE = []\n",
    "true_combined = ' '.join([article['text'] for article in true_sample])\n",
    "tags = pos_tag(word_tokenize(true_combined))\n",
    "chunks = ne_chunk(tags)\n",
    "n_chunks = len(chunks)\n",
    "for i, chunk in enumerate(chunks):\n",
    "    if hasattr(chunk, 'label'):\n",
    "        true_NE.append(' '.join([word[0] for word in chunk]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_fake_NE = Counter(fake_NE)\n",
    "counts_true_NE = Counter(true_NE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_tf = {}\n",
    "\n",
    "for token in counts_fake_NE:\n",
    "    count = counts_fake_NE[token]\n",
    "    if token in counts_true_NE:\n",
    "        fake_tf[token] = count / counts_true_NE[token]\n",
    "    else:\n",
    "        # pretends tokens not appearing in true appear once (to avoid dividing by zero)\n",
    "        fake_tf[token] = count\n",
    "\n",
    "\n",
    "# -------- sorts and prints tokens with highest value = most particular to fake news --------\n",
    "sorted_fake = sorted(list(fake_tf.items()), key=lambda x: x[1], reverse=True)\n",
    "for token, count in sorted_:\n",
    "    print(token, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tokens = 20\n",
    "\n",
    "x_fake = [token for token, _  in sorted_fake[:20]]\n",
    "y_fake = [articles['freq'] for _, articles in sorted_fake[:20]]\n",
    "\n",
    "x_fake.reverse()\n",
    "y_fake.reverse()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,12))\n",
    "\n",
    "ax.barh(x_fake, y_fake)\n",
    "ax.set_xticklabels(y_fake, rotation=50, rotation_mode='anchor', ha='right')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_tf = {}\n",
    "\n",
    "for token in counts_true_NE:\n",
    "    count = counts_true_NE[token]\n",
    "    if token in counts_fake_NE:\n",
    "        true_tf[token] = count / counts_fake_NE[token]\n",
    "    else:\n",
    "        # pretends tokens not appearing in fake appear once (to avoid dividing by zero)\n",
    "        true_tf[token] = count\n",
    "\n",
    "\n",
    "# -------- sorts and prints tokens with highest value = most particular to true news --------\n",
    "sorted_ = sorted(list(true_tf.items()), key=lambda x: x[1], reverse=True)\n",
    "for token, count in sorted_:\n",
    "    print(token, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tokens = 20\n",
    "\n",
    "x_true = [token for token, _  in sorted_true[:20]]\n",
    "y_true = [articles['freq'] for _, articles in sorted_true[:20]]\n",
    "\n",
    "x_true.reverse()\n",
    "y_true.reverse()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,12))\n",
    "\n",
    "ax.barh(x_true, y_true)\n",
    "ax.set_xticklabels(y_true, rotation=50, rotation_mode='anchor', ha='right')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_fake_NE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

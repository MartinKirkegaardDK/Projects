{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments,Trainer, pipeline\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from sklearn.metrics import classification_report\n",
    "from bertviz import model_view,head_view\n",
    "import shap\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from nltk import ne_chunk, pos_tag, word_tokenize\n",
    "import nltk\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clarify and reflect on the definition of the term \"fake news\", which may vary among databases, sometimes non-binary.\n",
    "Fake news has many definitions, ranging from being factually incorrect to misleading, which makes it hard when quantifying results and cross-examining results between different studies, since the latent space of a fitted model can be very different dependent on the definition of the task. The problem with using objective truth as the definition, that truth can vary depending on culture and context. An actor in a conflict can be seen as the good freedom fighter fighting an oppressive regime by one side, and as a terrorist on the other side. Lastly, the truth can also change over time, which means that models have to be retrained with up-to-date information constantly to be able to combat the fake news within the category factual correct. A conception of fake news based on 'clickbait' (as used in [1]) would be an inherently easier task since the model would not have to have as complex a world model in order to classify correctly. It should be solvable by using information only present in the text and comparing that to the title. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Research, where the data comes from and inspect the data: what are the labels, sources, and authors?\n",
    "The data used for this project is a fake news dataset which can be found on huggingface under that path: [GonzaloA/fake_news](https://huggingface.co/datasets/GonzaloA/fake_news). The data is described as a \"mix of other datasets which are the same scope, the Fake News\". Unfullfilled with this rather vague description, we sought to find additional information regarding the data and found this kaggle dataset: [ISOT Fake News Dataset](https://www.kaggle.com/datasets/emineyetm/fake-news-detection-datasets). We tested for similarity and found that **95%** of our training data titles are identical to data found in the kaggle dataset. An interesting outcome of the kaggle data is that all fake news articles come from **websites** flagged by Politifact and/or Wikipedia and not individual articles, and the true articles come from Reuters. This has the effect that de facto task our model is being trained for is whether or not an article is published by Reuters and not the intended fake news detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_data = pd.read_csv(\"Kaggle/Fake.csv\")\n",
    "true_data = pd.read_csv(\"Kaggle/True.csv\")\n",
    "\n",
    "our_data = load_dataset('GonzaloA/fake_news')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_data = pd.DataFrame(pd.concat([fake_data['title'],true_data['title']],axis = 0).reset_index(drop = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.DataFrame(our_data['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(train['title'],kaggle_data['title'],how = 'outer',indicator = True).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['_merge'] == 'both']['title'].count()/train['title'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Study the literature on how others approach this task. Check the related literature and select your model architecture of choice: LSTM, ...\n",
    "[Fake news detection based on news content and social contexts: a transformer-based approach](https://link.springer.com/article/10.1007/s41060-021-00302-z)\n",
    "\n",
    "The paper [1], written by Shaina Raza & Chen Ding, uses META's BART language model trained on two data sets: NELA-GT-19, which are news articles sourced from multiple sites, and Fakeddit, which is a multimodal dataset from Reddit, consisting of both images and text. The datasets used had more than a binary score, it included labels such as mixed, which is when there is a disagrement whether something is true or false, and categories such as satire into a single category Fake. They discuss their approach of continuously updating the model's training data to retrain the model and stay on top of relevant news. They also assert that freezing a model's weights can quickly make the model outdated since they don't generalize well to future events. Finally, they report an accuracy of 74.89%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] = https://link.springer.com/article/10.1007/s41060-021-00302-z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downloading the base model and getting the tokenizer\n",
    "The code below is not necessary to run, as we fine-tuned the model and uploaded it to HuggingFace, therefore just go down to the Model inspection part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"google-bert/bert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenize_function(examples):\n",
    "#     return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset('GonzaloA/fake_news')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.remove_columns(['Unnamed: 0','title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('predictions/predictions.txt','r') as f:\n",
    "    predictions = f.read().split('\\n')\n",
    "\n",
    "predictions = [int(i) for i in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_data = data.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling from the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small_train_dataset = tokenized_data[\"train\"].shuffle(seed=42).select(range(100))\n",
    "# small_eval_dataset = tokenized_data[\"validation\"].shuffle(seed=42).select(range(100))\n",
    "# small_test_dataset = tokenized_data['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outputting the training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_metrics(eval_pred):\n",
    "#     logits, labels = eval_pred\n",
    "#     predictions = np.argmax(logits, axis=-1)\n",
    "#     return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing the Trainer object and fine-tuning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=small_train_dataset,\n",
    "#     eval_dataset=small_eval_dataset,\n",
    "#     compute_metrics=compute_metrics,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_labels = small_test_dataset['label']\n",
    "# small_test_dataset = small_test_dataset.remove_columns(['label','token_type_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = trainer.predict(small_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted_labels = predictions.predictions.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.save_model('bert-base-uncased-fake-news-classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.save_pretrained('bert-base-uncased-fake-news-classification')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('predictions/predictions.txt', 'w') as f:\n",
    "#     for line in predicted_labels:\n",
    "#         f.write(f\"{line}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'FlorianMi/bert-base-uncased-fake-news-classification'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2,output_attentions = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(data['test']['label'],predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count = 0\n",
    "# for i,j in zip(predictions,data['test']['label']):\n",
    "#     if i != j:\n",
    "#         print(count)\n",
    "#     count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_dict = dict()\n",
    "for i in data:\n",
    "    true_news = []\n",
    "    fake_news = []\n",
    "    for j in data[i]['label']:\n",
    "        if j == 0:\n",
    "            fake_news.append(j)\n",
    "        else:\n",
    "            true_news.append(j)\n",
    "    count_dict[i] = (len(fake_news),len(true_news))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_df = pd.DataFrame.from_dict(count_dict,orient = 'index')\n",
    "count_df.rename(columns = {0:'Fake News',1:'True News'},inplace = True)\n",
    "count_df.plot.bar(figsize = (10,5));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_count_dict = dict()\n",
    "for i in data:\n",
    "    true_news = []\n",
    "    fake_news = []\n",
    "    for j in data[i]:\n",
    "        if j['label'] == 0:\n",
    "            fake_news.append(len(j['text']))\n",
    "        else:\n",
    "            true_news.append(len(j['text']))\n",
    "    len_count_dict[i] = (np.mean(fake_news),np.mean(true_news))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_count_df = pd.DataFrame.from_dict(len_count_dict,orient = 'index')\n",
    "len_count_df.rename(columns = {0:'Fake News',1:'True News'},inplace = True)\n",
    "len_count_df.plot.bar(figsize = (10,5));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspecting over-represented tokens and named entities in our corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ratios(first: list, other: list) -> dict:\n",
    "    '''\n",
    "    returns the frequency for each element in first divided by its frequency in other\n",
    "        - meant to show which elements are overrepresented in first compared to other\n",
    "    '''\n",
    "    first_counts = Counter(first)\n",
    "    other_counts = Counter(other)\n",
    "\n",
    "    ratios = {}\n",
    "\n",
    "    for item in first_counts:\n",
    "        if item in other_counts:\n",
    "            ratios[item] = first_counts[item] / other_counts[item]\n",
    "        else:\n",
    "            # to avoid division by zero, pretends items absent in other occur once instead\n",
    "            ratios[item] = first_counts[item] \n",
    "\n",
    "    return ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splits up data by label\n",
    "fake = []\n",
    "true = []\n",
    "for article in data['train']:\n",
    "    if article['label'] == 1: # label == true\n",
    "        true.append(article)\n",
    "    else:# label == fake\n",
    "        fake.append(article)\n",
    "\n",
    "\n",
    "# merges and tokenizes the text of the fake/non-fake news\n",
    "true_combined = ' '.join([article['text'] for article in true])\n",
    "fake_combined = ' '.join([article['text'] for article in fake])\n",
    "true_tokens = word_tokenize(true_combined)\n",
    "fake_tokens = word_tokenize(fake_combined)\n",
    "\n",
    "# computes token frequency in fake news / frequency in non-fake news (and vice versa) and sorts by ratio\n",
    "true_ratios = compute_ratios(true_tokens, fake_tokens)\n",
    "fake_ratios = compute_ratios(fake_tokens, true_tokens)\n",
    "true_ratios_sorted = sorted(list(true_ratios.items()), key=lambda x: x[1], reverse=True)\n",
    "fake_ratios_sorted = sorted(list(fake_ratios.items()), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepares token ratios for plotting\n",
    "\n",
    "# only plots top 10 highest ratios\n",
    "n_tokens = 10\n",
    "\n",
    "x_true = [token for token, _  in true_ratios_sorted[:n_tokens]]\n",
    "x_fake = [token for token, _  in fake_ratios_sorted[:n_tokens]]\n",
    "x_true.reverse()\n",
    "x_fake.reverse()\n",
    "\n",
    "y_true = [ratio for _, ratio in true_ratios_sorted[:n_tokens]]\n",
    "y_fake = [ratio for _, ratio in fake_ratios_sorted[:n_tokens]]\n",
    "y_true.reverse()\n",
    "y_fake.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples article to save compute time on the following\n",
    "size = 400\n",
    "fake_sample = np.random.choice(fake, size=size)\n",
    "true_sample = np.random.choice(true, size=size)\n",
    "\n",
    "# does NER on non-fake news, storing all named entities in list\n",
    "true_NE = []\n",
    "true_combined = ' '.join([article['text'] for article in true_sample])\n",
    "tags = pos_tag(word_tokenize(true_combined))\n",
    "chunks = ne_chunk(tags)\n",
    "n_chunks = len(chunks)\n",
    "for i, chunk in enumerate(chunks):\n",
    "    if hasattr(chunk, 'label'):\n",
    "        true_NE.append(' '.join([word[0] for word in chunk]))\n",
    "\n",
    "# does NER on fake news, storing all named entities in list\n",
    "fake_NE = []\n",
    "fake_combined = ' '.join([article['text'] for article in fake_sample])\n",
    "tags = pos_tag(word_tokenize(fake_combined))\n",
    "chunks = ne_chunk(tags)\n",
    "n_chunks = len(chunks)\n",
    "for i, chunk in enumerate(chunks):\n",
    "    if hasattr(chunk, 'label'):\n",
    "        fake_NE.append(' '.join([word[0] for word in chunk]))\n",
    "\n",
    "\n",
    "# computes named entity frequency in fake news / frequency in non-fake news (and vice versa) and sorts by ratio\n",
    "true_ratios_NE = compute_ratios(true_NE, fake_NE)\n",
    "fake_ratios_NE = compute_ratios(fake_NE, true_NE)\n",
    "true_ratios_sorted_NE = sorted(list(true_ratios_NE.items()), key=lambda x: x[1], reverse=True)\n",
    "fake_ratios_sorted_NE = sorted(list(fake_ratios_NE.items()), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepares named entity ratios for plotting\n",
    "\n",
    "# only plots top 10 highest ratios\n",
    "n_entities = 10\n",
    "\n",
    "x_true_NE = [entities for entities, _  in true_ratios_sorted_NE[:n_entities]]\n",
    "x_fake_NE = [entities for entities, _  in fake_ratios_sorted_NE[:n_entities]]\n",
    "x_true_NE.reverse()\n",
    "x_fake_NE.reverse()\n",
    "\n",
    "y_true_NE = [ratio for _, ratio in true_ratios_sorted_NE[:n_entities]]\n",
    "y_fake_NE = [ratio for _, ratio in fake_ratios_sorted_NE[:n_entities]]\n",
    "y_true_NE.reverse()\n",
    "y_fake_NE.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots all the ratios\n",
    "plt.style.use('dark_background')\n",
    "fig, axs = plt.subplots(figsize=(12,4), nrows=2, ncols=2)\n",
    "\n",
    "axs = axs.flatten()\n",
    "\n",
    "axs[0].barh(x_true, y_true)\n",
    "axs[0].set_title('Tokens overrepresented in non-fake news')\n",
    "axs[0].tick_params('y', labelsize=9)\n",
    "\n",
    "axs[1].barh(x_fake, y_fake)\n",
    "axs[1].set_title('Tokens overrepresented in fake news')\n",
    "axs[1].tick_params('y', labelsize=9)\n",
    "\n",
    "axs[2].barh(x_true_NE, y_true_NE)\n",
    "axs[2].set_title('Named Entities overrepresented in non-fake news')\n",
    "axs[2].tick_params('y', labelsize=9)\n",
    "\n",
    "axs[3].barh(x_fake_NE, y_fake_NE)\n",
    "axs[3].set_title('Named Entities overrepresented in fake news')\n",
    "axs[3].tick_params('y', labelsize=9)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig('images/overrepresented.png', dpi=300)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(100)\n",
    "\n",
    "print(true[np.random.randint(1, len(true))]['text'])\n",
    "print()\n",
    "print(true[np.random.randint(1, len(true))]['text'])\n",
    "print()\n",
    "print(true[np.random.randint(1, len(true))]['text'])\n",
    "print()\n",
    "print(true[np.random.randint(1, len(true))]['text'])\n",
    "print()\n",
    "print(true[np.random.randint(1, len(true))]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(100)\n",
    "\n",
    "print(fake[np.random.randint(1, len(fake))]['text'])\n",
    "print()\n",
    "print(fake[np.random.randint(1, len(fake))]['text'])\n",
    "print()\n",
    "print(fake[np.random.randint(1, len(fake))]['text'])\n",
    "print()\n",
    "print(fake[np.random.randint(1, len(fake))]['text'])\n",
    "print()\n",
    "print(fake[np.random.randint(1, len(fake))]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspecting the model and the head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.encode(data['test'][9]['text'], return_tensors='pt')\n",
    "outputs = model(inputs)\n",
    "attention = outputs[-1]  # Output includes attention weights when output_attentions=True\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# head_view(attention, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_view(attention, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, article in enumerate(data['test']):\n",
    "#     if len(article['text']) < 500 and article['label'] == 0:\n",
    "#         print(i)\n",
    "#         print(article['text'])\n",
    "#         print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspecting the model's attention\n",
    "Label 0 is Fake | Label 1 is True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline('text-classification',model=model_name, top_k=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.Explainer(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values_true = explainer([data['test'][15]['text']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.text(shap_values_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values_fake = explainer([data['test'][274]['text']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.text(shap_values_fake)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
